{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "Read data and divaded to three part train, valid and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert images to tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6680/6680 [00:26<00:00, 256.11it/s]\n",
      "100%|██████████| 835/835 [00:03<00:00, 227.94it/s]\n",
      "100%|██████████| 836/836 [00:03<00:00, 238.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Dogs Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG19_model():\n",
    "    # create the base pre-trained model\n",
    "    base_model = VGG19(weights='imagenet', include_top=False)\n",
    "    \n",
    "    # freeze all the layer in the model\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # let's add a fully-connected layer\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    # and a logistic layer \n",
    "    predictions = Dense(len(dog_names), activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.summary()\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model with Imagenet Wight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 3, None, None)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, None, None)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, None, None)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, None, None)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, None, None)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, None, None)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 128, None, None)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 256, None, None)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 256, None, None)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 256, None, None)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 256, None, None)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 256, None, None)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 512, None, None)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 512, None, None)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 512, None, None)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 512, None, None)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 512, None, None)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 512, None, None)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 512, None, None)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 512, None, None)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 512, None, None)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 512, None, None)   0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 133)               136325    \n",
      "=================================================================\n",
      "Total params: 20,686,021\n",
      "Trainable params: 661,637\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_imagenet.hdf5',\n",
    "                               verbose=1, save_best_only=True)\n",
    "tensorborad = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, \n",
    "            write_images=True, embeddings_freq=0, embeddings_layer_names=None,\n",
    "            embeddings_metadata=None, embeddings_data=None)\n",
    "model = VGG19_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User freezed VGG19 model\n",
    "This model is just traing the last two Dense, the VGG19 model's layers is freezed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/50\n",
      "6680/6680 [==============================] - 47s 7ms/step - loss: 4.7593 - val_loss: 4.5696\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.56957, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/50\n",
      "6680/6680 [==============================] - 44s 7ms/step - loss: 4.2350 - val_loss: 4.2091\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.56957 to 4.20911, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 3.7589 - val_loss: 3.8376\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.20911 to 3.83763, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 4/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 3.3927 - val_loss: 3.7727\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.83763 to 3.77269, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 5/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 3.1172 - val_loss: 3.5473\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.77269 to 3.54732, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 6/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 2.8994 - val_loss: 3.6804\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3.54732\n",
      "Epoch 7/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 2.7304 - val_loss: 3.2333\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.54732 to 3.23327, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 8/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 2.5858 - val_loss: 3.3021\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3.23327\n",
      "Epoch 9/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 2.4580 - val_loss: 3.0577\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.23327 to 3.05773, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 10/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 2.3504 - val_loss: 3.1951\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.05773\n",
      "Epoch 11/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 2.2539 - val_loss: 2.8915\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.05773 to 2.89151, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 12/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 2.1691 - val_loss: 2.9542\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.89151\n",
      "Epoch 13/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 2.0960 - val_loss: 3.1949\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.89151\n",
      "Epoch 14/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 2.0228 - val_loss: 2.9262\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.89151\n",
      "Epoch 15/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 1.9443 - val_loss: 3.0274\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.89151\n",
      "Epoch 16/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 1.8865 - val_loss: 2.8572\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.89151 to 2.85725, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 17/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 1.8269 - val_loss: 3.0304\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.85725\n",
      "Epoch 18/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 1.7818 - val_loss: 2.9721\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.85725\n",
      "Epoch 19/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 1.7285 - val_loss: 2.9953\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.85725\n",
      "Epoch 20/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 1.6655 - val_loss: 2.9934\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.85725\n",
      "Epoch 21/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 1.6215 - val_loss: 2.8272\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.85725 to 2.82722, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 22/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 1.5755 - val_loss: 2.9234\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.82722\n",
      "Epoch 23/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 1.5285 - val_loss: 3.0035\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.82722\n",
      "Epoch 24/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 1.4892 - val_loss: 2.9294\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.82722\n",
      "Epoch 25/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 1.4551 - val_loss: 3.0646\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.82722\n",
      "Epoch 26/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 1.4110 - val_loss: 3.5821\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.82722\n",
      "Epoch 27/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 1.3780 - val_loss: 3.1038\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.82722\n",
      "Epoch 28/50\n",
      "6680/6680 [==============================] - 45s 7ms/step - loss: 1.3344 - val_loss: 2.9530\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.82722\n",
      "Epoch 29/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 1.2883 - val_loss: 3.1347\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.82722\n",
      "Epoch 30/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 1.2619 - val_loss: 3.0644\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.82722\n",
      "Epoch 31/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 1.2253 - val_loss: 3.1120\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.82722\n",
      "Epoch 32/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 1.1880 - val_loss: 2.7550\n",
      "\n",
      "Epoch 00032: val_loss improved from 2.82722 to 2.75500, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 33/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 1.1612 - val_loss: 3.1275\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2.75500\n",
      "Epoch 34/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 1.1202 - val_loss: 3.1574\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2.75500\n",
      "Epoch 35/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 1.0878 - val_loss: 2.8808\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 2.75500\n",
      "Epoch 36/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 1.0577 - val_loss: 3.3324\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 2.75500\n",
      "Epoch 37/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 1.0278 - val_loss: 3.0471\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 2.75500\n",
      "Epoch 38/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 1.0003 - val_loss: 3.0334\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2.75500\n",
      "Epoch 39/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 0.9698 - val_loss: 3.2763\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 2.75500\n",
      "Epoch 40/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 0.9289 - val_loss: 2.9549\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 2.75500\n",
      "Epoch 41/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 0.9165 - val_loss: 3.3187\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2.75500\n",
      "Epoch 42/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 0.8878 - val_loss: 3.1179\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 2.75500\n",
      "Epoch 43/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 0.8536 - val_loss: 3.4043\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2.75500\n",
      "Epoch 44/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 0.8289 - val_loss: 2.9438\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.75500\n",
      "Epoch 45/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 0.7994 - val_loss: 3.6522\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.75500\n",
      "Epoch 46/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 0.7845 - val_loss: 3.4096\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2.75500\n",
      "Epoch 47/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 0.7502 - val_loss: 3.3521\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2.75500\n",
      "Epoch 48/50\n",
      "6680/6680 [==============================] - 47s 7ms/step - loss: 0.7236 - val_loss: 2.9449\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.75500\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 46s 7ms/step - loss: 0.7005 - val_loss: 3.7858\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.75500\n",
      "Epoch 50/50\n",
      "6680/6680 [==============================] - 46s 7ms/step - loss: 0.6732 - val_loss: 3.3696\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.75500\n"
     ]
    }
   ],
   "source": [
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit(train_tensors, train_targets,validation_data=(valid_tensors,valid_targets), \n",
    "                    epochs=epochs, batch_size=75, callbacks=[checkpointer,tensorborad], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the result for after train freezed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning accuracy: 62.3952%\n",
      "Validation accuracy: 62.3952%\n",
      "Test accuracy: 32.6555%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')\n",
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "dog_breed_predictions_train=[np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in train_tensors]\n",
    "# report train accuracy\n",
    "train_accuracy = 100*np.sum(np.array(dog_breed_predictions_train)==np.argmax(train_targets, axis=1))/len(dog_breed_predictions_train)\n",
    "print('traning accuracy: %.4f%%' % train_accuracy)\n",
    "\n",
    "dog_breed_predictions_valid=[np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in valid_tensors]\n",
    "# report valid accuracy\n",
    "validation_accuracy = 100*np.sum(np.array(dog_breed_predictions_valid)==np.argmax(valid_targets, axis=1))/len(dog_breed_predictions_valid)\n",
    "print('Validation accuracy: %.4f%%' % train_accuracy)\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set trainable value\n",
    "change the trainable value for the first 5 and retrain the other layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 3, None, None)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, None, None)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, None, None)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, None, None)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, None, None)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, None, None)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 128, None, None)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 256, None, None)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 256, None, None)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 256, None, None)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 256, None, None)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 256, None, None)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 512, None, None)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 512, None, None)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 512, None, None)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 512, None, None)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 512, None, None)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 512, None, None)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 512, None, None)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 512, None, None)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 512, None, None)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 512, None, None)   0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 133)               136325    \n",
      "=================================================================\n",
      "Total params: 20,686,021\n",
      "Trainable params: 661,637\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = VGG19_model()\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 6 layers and unfreeze the rest:\n",
    "for layer in model.layers[:6]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "for layer in model.layers[6:]:\n",
    "    layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/1000\n",
      "6680/6680 [==============================] - 97s 15ms/step - loss: 15.9220 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.75500\n",
      "Epoch 2/1000\n",
      "6680/6680 [==============================] - 99s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.75500\n",
      "Epoch 3/1000\n",
      "6680/6680 [==============================] - 99s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.75500\n",
      "Epoch 4/1000\n",
      "6680/6680 [==============================] - 99s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.75500\n",
      "Epoch 5/1000\n",
      "6680/6680 [==============================] - 99s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.75500\n",
      "Epoch 6/1000\n",
      "6680/6680 [==============================] - 99s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.75500\n",
      "Epoch 7/1000\n",
      "6680/6680 [==============================] - 99s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.75500\n",
      "Epoch 8/1000\n",
      "6680/6680 [==============================] - 99s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.75500\n",
      "Epoch 9/1000\n",
      "6680/6680 [==============================] - 99s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.75500\n",
      "Epoch 10/1000\n",
      "6680/6680 [==============================] - 100s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.75500\n",
      "Epoch 11/1000\n",
      "6680/6680 [==============================] - 100s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.75500\n",
      "Epoch 12/1000\n",
      "6680/6680 [==============================] - 100s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.75500\n",
      "Epoch 13/1000\n",
      "6680/6680 [==============================] - 101s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.75500\n",
      "Epoch 14/1000\n",
      "6680/6680 [==============================] - 102s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.75500\n",
      "Epoch 15/1000\n",
      "6680/6680 [==============================] - 101s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.75500\n",
      "Epoch 16/1000\n",
      "6680/6680 [==============================] - 104s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.75500\n",
      "Epoch 17/1000\n",
      "6680/6680 [==============================] - 102s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.75500\n",
      "Epoch 18/1000\n",
      "6680/6680 [==============================] - 100s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.75500\n",
      "Epoch 19/1000\n",
      "6680/6680 [==============================] - 102s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.75500\n",
      "Epoch 20/1000\n",
      "6680/6680 [==============================] - 104s 16ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.75500\n",
      "Epoch 21/1000\n",
      "6680/6680 [==============================] - 102s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.75500\n",
      "Epoch 22/1000\n",
      "6680/6680 [==============================] - 100s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.75500\n",
      "Epoch 23/1000\n",
      "6680/6680 [==============================] - 103s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.75500\n",
      "Epoch 24/1000\n",
      "6680/6680 [==============================] - 105s 16ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.75500\n",
      "Epoch 25/1000\n",
      "6680/6680 [==============================] - 103s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.75500\n",
      "Epoch 26/1000\n",
      "6680/6680 [==============================] - 100s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.75500\n",
      "Epoch 27/1000\n",
      "6680/6680 [==============================] - 100s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.75500\n",
      "Epoch 28/1000\n",
      "6680/6680 [==============================] - 101s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.75500\n",
      "Epoch 29/1000\n",
      "6680/6680 [==============================] - 104s 16ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.75500\n",
      "Epoch 30/1000\n",
      "6680/6680 [==============================] - 104s 16ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.75500\n",
      "Epoch 31/1000\n",
      "6680/6680 [==============================] - 101s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.75500\n",
      "Epoch 32/1000\n",
      "6680/6680 [==============================] - 101s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2.75500\n",
      "Epoch 33/1000\n",
      "6680/6680 [==============================] - 102s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2.75500\n",
      "Epoch 34/1000\n",
      "6680/6680 [==============================] - 101s 15ms/step - loss: 15.9661 - val_loss: 15.9637\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2.75500\n",
      "Epoch 35/1000\n",
      " 740/6680 [==>...........................] - ETA: 1:25 - loss: 15.9874"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "history=model.fit(train_tensors, train_targets,validation_data=(valid_tensors,valid_targets), \n",
    "                    epochs=epochs, batch_size=20, callbacks=[checkpointer,tensorborad], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline \n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning accuracy: 62.3952%\n",
      "Validation accuracy: 62.3952%\n",
      "Test accuracy: 32.6555%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')\n",
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "dog_breed_predictions_train=[np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in train_tensors]\n",
    "# report train accuracy\n",
    "train_accuracy = 100*np.sum(np.array(dog_breed_predictions_train)==np.argmax(train_targets, axis=1))/len(dog_breed_predictions_train)\n",
    "print('traning accuracy: %.4f%%' % train_accuracy)\n",
    "\n",
    "dog_breed_predictions_valid=[np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in valid_tensors]\n",
    "# report valid accuracy\n",
    "validation_accuracy = 100*np.sum(np.array(dog_breed_predictions_valid)==np.argmax(valid_targets, axis=1))/len(dog_breed_predictions_valid)\n",
    "print('Validation accuracy: %.4f%%' % train_accuracy)\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
